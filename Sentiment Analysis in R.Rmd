---
title: "Sentiment Analysis in R"
subtitle: "DataCamp Course by Ted Kwartler"
author: "Laurent Barcelo"
date: "September 25, 2017"
output: 
  html_notebook:
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

# 1st Segment - Fast & Dirty: Polarity scoring

## Jump right in! Visualize polarity

Sentiment analysis helps you extract an author's feelings towards a subject. This exercise will give you a taste of what's to come!

We created `text_df` representing a conversation with `person` and `text` columns.

Use `qdap`'s `polarity()` function to score `text_df`. `polarity()` will accept a single character object or data frame with a grouping variable to calculate a positive or negative score.

In this example you will use the `magrittr` package's dollar pipe operator `%$%`. The dollar sign forwards the data frame into `polarity()` and you declare a text column name or the text column and a grouping variable without quotes.

> text_data_frame %$% polarity(text_column_name)

To create an object with the dollar sign operator:

> polarity_object <- text_data_frame %$%   
>  polarity(text_column_name, grouping_column_name)

More specifically, to make a quantitative judgement about the sentiment of some text, you need to give it a score. A simple method is a positive or negative value related to a sentence, passage or a collection of documents called a corpus. Scoring with positive or negative values only is called "polarity." A useful function for extracting polarity scores is `counts()` applied to the polarity object. For a quick visual call `plot()` on the `polarity()` outcome.

#### Instructions
* Examine the `text_df` conversation data frame.
* Using `%$%` pass `text_df` to `polarity()` along with the column name text without quotes. This will print the polarity for all text.
* Create a new object `datacamp_conversation` by forwarding `text_df` with `%$%` to `polarity()`. Pass in text followed by the grouping person column. This will calculate polarity according to each individual person. Since it is all within parentheses the result will be printed too.
* Apply `counts()` to datacamp_conversation to print the specific emotional words that were found.
* `plot()` the datacamp_conversation.

```{r echo = F}
text_df <- data.frame(matrix(NA, nrow = 8, ncol = 2))
text_df[,1] <- as.factor(c("Nick ", "Jonathan", "Martijn",  "Nicole", "Nick", "Jonathan", "Martijn", "Nicole"))
text_df[,2] <- as.factor(c("DataCamp courses are the best", "I like talking to students", "Other online data science curricula are boring.", "What is for lunch?", "DataCamp has lots of great content!", "Students are passionate and are excited to learn", "Other data science curriculum is hard to learn and difficult to understand", "I think the food here is good."))
colnames(text_df) <- c("person", "text")
```

```{r}
library(magrittr)
library(qdap)
library(tidyverse)
text_df

text_df %$% polarity(text)

# Calc polarity score by person
(datacamp_conversation <- text_df %$% polarity(text, person))

counts(datacamp_conversation)

plot(datacamp_conversation)
```

## TM refresher (I)

In the Text Mining: Bag of Words course you learned that a corpus is a set of texts, and you studied some functions for preprocessing the text. To recap, one way to create a corpus is with the functions below. Even though this is a different course, sentiment analysis is part of text mining so a refresher can be helpful.

* Turn a character vector into a text source using `VectorSource()`.
* Turn a text source into a corpus using `VCorpus()`.
* Remove unwanted characters from the corpus using cleaning functions like `removePunctuation()` and `stripWhitespace()` from `tm`, and `replace_abbreviation()` from `qdap`.

In this exercise a custom `clean_corpus()` function has been created using standard preprocessing functions for easier application.

`clean_corpus()` accepts the output of `VCorpus()` and applies cleaning functions. For example:

> processed_corpus <- clean_corpus(my_corpus)

#### Instructions
Your R session has a text vector, `tm_define`, containing two small documents and the function `clean_corpus()`.

* Create an object called `tm_vector` by applying `VectorSource()` to `tm_define`.
* Make `tm_corpus` using `VCorpus()` on `tm_vector`.
* Use `content()` to examine the contents of the first document in `tm_corpus`.
  + Documents in the corpus are accessed using list syntax, so use double square brackets, e.g. `[[1]]`.
* Clean the corpus text using the custom function `clean_corpus()` on `tm_corpus`. Call this new object `tm_clean`.
* Examine the first document of the new `tm_clean` object again to see how the text changed after `clean_corpus()` was applied.

```{r}
# data prep
library(tm)
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee"))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

tm_define <- c("Text mining is the process of distilling actionable insights from text.", "Sentiment analysis represents the set of tools to extract an author's feelings towards a subject.")

tm_vector <- VectorSource(tm_define)
tm_corpus <- VCorpus(tm_vector)
content(tm_corpus[[1]])
tm_clean <- clean_corpus(tm_corpus)
content(tm_clean[[1]])
```

## TM refresher (II)

Now let's create a Document Term Matrix (DTM). In a DTM:

* Each row of the matrix represents a document.
* Each column is a unique word token.
* Values of the matrix correspond to an individual document's word usage.

The DTM is the basis for many bag of words analyses. Later in the course, you will also use the related Term Document Matrix (TDM). This is the transpose; that is, columns represent documents and rows represent unique word tokens.

You should construct a DTM after cleaning the corpus (using `clean_corpus()`). To do so, call `DocumentTermMatrix()` on the corpus object.

> tm_dtm <- DocumentTermMatrix(tm_clean

If you need a more in-depth refresher check out the Text Mining: Bag of Words course. Hopefully these two exercises have prepared you well enough to embark on your sentiment analysis journey!

#### Instructions
We've created a `VCorpus()` object called `clean_text` containing 1000 tweets mentioning coffee. The tweets have been cleaned with the previously mentioned preprocessing steps and your goal is to create a DTM from it.

* Apply `DocumentTermMatrix()` to the `clean_text` corpus to create a term frequency weighted DTM called `tf_dtm`.
* Change the `DocumentTermMatrix()` object into a simple matrix with `as.matrix()`. Call the new object `tf_dtm_m`.
* Check the dimensions of the matrix using `dim()`.
* Use square bracket indexing to see a subset of the matrix.
  + Select rows 16 to 20, and columns 2975 to 2985
* Note the frequency value of the word "working."

```{r}
# data prep
coffee <- read_csv("/Users/lbarcelo/R_Repo/Rdatasets/DataCamp/Text Minings - Bags of Words/coffee.csv")
coffee_tweets <- coffee$text
coffee_source <- VectorSource(coffee_tweets)
coffee_corpus <- VCorpus(coffee_source)
clean_text <- clean_corpus(coffee_corpus)

clean_text
tf_dtm <- DocumentTermMatrix(clean_text)
tf_dtm_m <- as.matrix(tf_dtm)
dim(tf_dtm_m)
tf_dtm_m[16:20, 2975:2985]
tf_dtm_m[1:200, "working"]
```

## Where can you observe Zipf's law?

Although Zipf observed a steep and predictable decline in word usage you may not buy into Zipf's law. You may be thinking "I know plenty of words, and have a distinctive vocabulary". That may be the case, but the same can't be said for most people! To prove it, let's construct a visual from 3 million tweets mentioning "#sb". Keep in mind that the visual doesn't follow Zipf's law perfectly, the tweets all mentioned the same hashtag so it is a bit skewed. That said, the visual you will make follows a steep decline showing a small lexical diversity among the millions of tweets. So there is some science behind using lexicons for natural language analysis!

In this exercise, you will use the package `metricsgraphics`. Although the author suggests using the pipe `%>%` operator, you will construct the graphic step-by-step to learn about the various aspects of the plot. The main function of the package `metricsgraphics` is the `mjs_plot()` function which is the first step in creating a JavaScript plot. Once you have that, you can add other layers on top of the plot.

An example `metricsgraphics` workflow without using the `%>%` operator is below:

> metro_plot <- mjs_plot(data, x = x_axis_name, y = y_axis_name, show_rollover_text = FALSE)  
> metro_plot <- mjs_line(metro_plot)  
> metro_plot <- mjs_add_line(metro_plot, line_one_values)  
> metro_plot <- mjs_add_legend(metro_plot, legend = c('names', 'more_names'))  
> metro_plot

#### Instructions
* Use `head()` on sb_words to review top words.
* Create a new column `expectations` by dividing the largetst word frequency, `freq[1]`, by the `rank` column.
* Start `sb_plot` using `mjs_plot()`.
  + Pass in `sb_words` with `x = rank` and `y = freq`.
  + Within `mjs_plot()` set `show_rollover_text` to `FALSE`.
* Overwrite `sb_plot` using `mjs_line()` and pass in `sb_plot`.
* Add to `sb_plot` with `mjs_add_line()`.
  + Pass in the previous `sb_plot` object and the vector, `expectations`.
* Place a legend on a new `sb_plot` object using `mjs_add_legend()`.
  + Pass in the previous sb_plot object
  + The legend labels should consist of `"Frequency"` and `"Expectation"`.
* Call `sb_plot` to display the plot. Mouseover a point to simultaneously highlight a `freq` and `Expectation` point. The magic of JavaScript!

```{r}
# Data (sb_words not available)
# Examine sb_words
head(sb_words)

# Create expectations
sb_words$expectations <- sb_words %$% {freq[1] / rank}

# Create metrics plot
sb_plot <- mjs_plot(sb_words, x = rank, y = freq, show_rollover_text = F)

# Add 1st line
sb_plot <- mjs_line(sb_plot)
####
# Add 2nd line
sb_plot <- mjs_add_line(sb_plot, expectations)

# Add legend
sb_plot <- mjs_add_legend(sb_plot, legend = c("Frequency", "Expectation"))

# Display plot
sb_plot
```

## Polarity on actual text

So far you have learned the basic components needed for assessing positive or negative intent in text. Remember the following points so you can feel confident in your results.

The subjectivity lexicon is a predefined list of words associated with emotions or positive/negative feelings.
You don't have to list every word in a subjectivity lexicon because Zipf's law describes human expression.
A quick way to get started is to use the `polarity()` function which has a built-in subjectivity lexicon.

The function scans the text to identify words in the lexicon. It then creates a word group around the identified positive or negative subjectivity word. Within the group **valence shifters** adjust the score. Valence shifters are words that amplify or negate the emotional intent of the subjectivity word. For example, "well known" is positive while "not well known" is negative. Here "not" is a negating term and reverses the emotional intent of "well known." In contrast, "very well known" employs an amplifier increasing the positive intent.

The `polarity()` function then calculates a score using subjectivity terms, valence shifters and the total number of words in the passage. This exercise demonstrates a simple polarity calculation. In the next video we look under the hood of `polarity()` for more detail.

#### Instructions
* Calculate the `polarity()` of positive in a new object called pos_score. Encase the entire call in parentheses so the output is also printed.

Manually perfrom the same polarity calculation.

* Get a word count object by calling `counts()` on the polarity object.
* All the identified subjectivity words are part of count object's list. Specifically, positive words are in `$pos.words` element vector. Find the number of positive words in `n_good` by calling `length()` on the first part of the `$pos.words` element.
* Capture the total number of words and assign it to n_words. This value is stored in pos_count as the wc element.
* Deconstruct the polarity() calculation by dividing n_good by sqrt() of n_words. Compare the result to pos_pol to the equation's result.

```{r}
# Example statement
positive <- "DataCamp courses are good for learning"

# Calculate polarity of statement
(pos_score <- polarity(positive))

(pos_counts <- counts(pos_score))

# Number of positive words
n_good <- length(pos_counts$pos.words[[1]])

# Total number of words
n_words <- pos_counts$wc
  
# Verify polarity score
n_good / sqrt(n_words)
```

## Happy songs!

Of course just positive and negative words aren't enough. In this exercise you will learn about valence shifters which tell you about the author's emotional intent. Previously you applied `polarity()` to text without valence shifters. In this example you will see amplifers and negating words in action.

Recall that an amplifying word adds 0.8 to a positive word in `polarity()` so the positive score becomes 1.8. For negative words 0.8 is subtracted so the total becomes -1.8. Then the score is divided by the square root of the total number of words.

Consider the following example from Frank Sinatra:

* **"It was a very good year"**

"Good" equals 1 and "very" adds another 0.8. So, 1.8/sqrt(6) results in 0.73 polarity.

A negating word such as "not" will inverse the subjectivity score. Consider the following example from Bobby McFerrin:

* **"Don't worry Be Happy"**

"worry is now 1 due to the negation "don't." Adding the "happy", +1, equals 2. With 4 total words, 2 / sqrt(4) equals a polarity score of 1.

#### Instructions
* Examine the conversation data frame, `conversation`. Note the valence shifters like `"never"` in the text column.
* Apply `polarity()` to the text column of conversation to calculate polarity for the entire conversation.
* Calculate the polarity scores by student, assigning the result to `student_pol`.
  + Call `polarity()` again, this time passing two columns of conversation.
  + The text variable is text and the grouping variable is student.
* To see the student level results, use `scores()` on `student_pol`.
* The `counts()` function applied to `student_pol` will print the sentence level polarity for the entire data frame along with lexicon words identified.
* The polarity object, `student_pol`, can be plotted with `plot()`.

```{r}
# data prep
conversation <- data.frame(matrix(NA, nrow = 3, ncol = 2))
conversation[,1] <- as.factor(c("Martijn", "Nick",  "Nicole"))
conversation[,2] <- as.factor(c("This restaurant is never bad", "The lunch was very good", "It was awful I got food poisoning and was extremely ill"))
colnames(conversation) <- c("student", "text")

conversation

conversation %$% polarity(text)
(student_pol <- conversation %$% polarity(text, student))

scores(student_pol) # same as printing it?

counts(student_pol)

plot(student_pol)
```

## LOL, this song is wicked good

Even with Zipf's law in action, you will still need to adjust lexicons to fit the text source (for example twitter versus legal documents) or the author's demographics (teenage girl versus middle aged man). This exercise demonstrates the explicit components of `polarity()` so you can change it if needed.

In Trey Songz "Lol :)" song there is a lyric "LOL smiley face, LOL smiley face." In the basic `polarity()` function, "LOL" is not defined as positive. However, "LOL" stands for "Laugh Out Loud" and should be positive. As a result, you should adjust the lexicon to fit the text's context which includes pop-culture slang. If your analysis contains text from a specific channel (Twitter's "LOL"), location (Boston's "Wicked Good"), or age group (teenagers "sick") you will likely have to adjust the lexicon.

In this exercise you are not adjusting the subjectivity lexicon or qdap dictionaries containing valence shifters. Instead you are examining the existing word data frame objects so you can change them in the following exercise.

We've created `text` containing two excerpts from Beyoncé's "Crazy in Love" lyrics for the exercise.

#### Instructions
* Print `key.pol` to see a portion of the subjectivity words and values.
* Examine the predefined `negation.words` to print all the negating terms.
* Now print the `amplification.words` to see the words that add values to the lexicon.
* Check the `deamplification.words` to print the words that reduce the lexicon values.
* Call `text` to see conversation.
* Calculate polarity() as follows.
  + Set `text.var` to `text$words`.
  + Set `grouping.var` to `text$speaker`.
  + Set `polarity.frame` to `key.pol`.
  + Set `negators` to `negation.words`.
  + Set `amplifiers` to `amplification.words`.
  + Set `deamplifiers` to `deamplification.words`.

```{r}
# Examine the key.pol
key.pol

# Negators
negation.words

# Amplifiers
amplification.words

# De-amplifiers
deamplification.words

# data prep
text <- data.frame(matrix(NA, nrow = 2, ncol = 2))
text[,1] <- as.factor(c("beyonce", "jay_z"))
text[,2] <- as.factor(c("I know I dont understand Just how your love can do what no one else can", "They cant figure him out they like hey, is he insane"))
colnames(text) <- c("speaker", "words")

text

polarity(text.var = text$words,
         grouping.var = text$speaker,
         polarity.frame = key.pol,
         negators = negation.words,
         amplifiers = amplification.words,
         deamplifiers = deamplification.words)

```

## Stressed Out!

Here you will adjust the negative words to account for the specific text. You will then compare the basic and custom `polarity()` scores.

A popular song from Twenty One Pilots is called "Stressed Out". If you scan the lyrics of this song, you will observe the song is about youthful nostalgia. Overall, most people would say the polarity is negative. Repeatedly the lyrics mention stress, fears and pretending.

Let's compare the song lyrics using the default subjectivity lexicon and also a custom one.

To start, you need to verify the `key.pol` subjectivity lexicon does not already have the term you want to add. One way to check is with `grep()`. The `grep()` function returns the row containing characters that match a search pattern. Here is an example used while indexing.

> data_frame[grep("search_pattern", data_frame$column), ]

After verifying the slang or new word is not already in the `key.pol` lexicon you need to add it. The code below uses `sentiment_frame()` to construct the new lexicon. Within the code `sentiment_frame()` accepts the original positive word vector, `positive.words`. Next, the original `negative.words` are concatenated to "smh" and "kappa", both considered negative slang. Although you can declare the positive and negative weights, the default is 1 and -1 so they are not included below.

> custom_pol <- sentiment_frame(positive.words, c(negative.words, "hate", "pain"))

Now you are ready to apply polarity and it will reference the *custom* subjectivity lexicon!

#### Instructions
We've created `stressed_out` which contains the lyrics to the song "Stressed Out", by Twenty One Pilots.

* Use `polarity()` on stressed_out to see the default score.
* Check `key.pol` for any words containing "stress". Use `grep()` to index the data frame by searching in the `x` column.
* Create `custom_pol` as a new sentiment data frame.
  + Call `sentiment_frame()` and pass `positive.words` as the first argument without concatenating any new terms.
  + Next, use `c()` to combine `negative.words` with new terms `"stressed"` and `"turn back"`.
* Reapply `polarity()` to `stressed_out` with the additional parameter `polarity.frame = custom_pol` to compare how the new words change the score to a more accurate representation of the song.

```{r}
# data prep
stressed_out <- "I wish I found some better sounds no ones ever heard\nI wish I had a better voice that sang some better words\nI wish I found some chords in an order that is new\nI wish I didnt have to rhyme every time I sang\nI was told when I get older all my fears would shrink\nBut now Im insecure and I care what people think\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWere stressed out\nSometimes a certain smell will take me back to when I was young\nHow come Im never able to identify where its coming from\nId make a candle out of it if I ever found it\nTry to sell it never sell out of it Id probably only sell one\nItd be to my brother, cause we have the same nose\nSame clothes homegrown a stones throw from a creek we used to roam\nBut it would remind us of when nothing really mattered\nOut of student loans and tree-house homes we all would take the latter\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWe used to play pretend, give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face #\nSaying, Wake up you need to make money\nYeah\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nUsed to play pretend, used to play pretend bunny\nWe used to play pretend wake up, you need the money\nUsed to play pretend used to play pretend bunny\nWe used to play pretend, wake up, you need the money\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah"

head(stressed_out)

polarity(stressed_out)

key.pol[str_detect(key.pol$x, "stress")] #stringr version
key.pol[grep("stress", x)] # base version

custom_pol <- sentiment_frame(positive.words, c(negative.words, "stressed", "turn back"))

polarity(text.var = stressed_out, polarity.frame = custom_pol)
```

# 2nd Segment - Sentiment analysis the tidytext way

<img src="/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Resources/Capture d’écran 2018-09-25 à 17.34.36.png" alt="Plutchik's Wheel of Emotion" width="600"/>

## DTM vs. tidytext matrix

The [tidyverse](https://www.tidyverse.org) is a collection of R packages that share common philosophies and are designed to work together. This chapter covers some tidy functions to manipulate data. In this exercise you will compare a DTM to a tidy text data frame called a tibble.

Within the tidyverse, each observation is a single row in a data frame. That makes working in different packages much easier since the fundamental data structure is the same. Parts of this course borrow heavily from the `tidytext` package which uses this data organization.

For example, you may already be familiar with the `%>%` operator from the magrittr package. This forwards an object on its left-hand side as the first argument of the function on its right-hand side.

In the example below, you are forwarding the `data` object to `function1()`. Notice how the parentheses are empty. This in turn is forwarded to `function2()`. In the last function you don't have to add the data object because it was forwarded from the output of `function1()`. However, you do add a fictitious parameter, some_parameter as `TRUE`. These pipe forwards ultimately create the object.

> object <- data %>%   
>            function1() %>%  
>            function2(some_parameter = TRUE)

To use the `%>%` operator, you don't necessarily need to load the `magrittr` package, since it is also available in the `dplyr` package. dplyr also contains the functions `inner_join()` (which you'll learn more about later) and `count()` for tallying data. The last function you'll need is `mutate()` to create new variables or modify existing ones.

> object <- data %>%  
>  mutate(new_Var_name = Var1 - Var2)

or to modify a variable

> object <- data %>%  
>  mutate(Var1 = as.factor(Var1))
  
You will also use `tidyr`'s `spread()` function to organize the data with each row being a line from the book and the positive and negative values as columns.

**index** |	**negative** |	**positive**
---|---|---
42 |	2  |	0
43 |	0	 |  1
44 |	1	 |  0

To change a `DTM` to a tidy format use `tidy()` from the `tiditext` package.

> tidy_format <- tidy(Document_Term_Matrix)

This exercise uses text from the Greek tragedy, *Agamemnon*. *Agamemnon* is a story about marital infidelity and murder. You can download a copy [here](http://www.gutenberg.org/ebooks/14417?msg=welcome_stranger).

#### Instructions
We've already created a clean DTM called `ag_dtm` for this exercise.

* Create `ag_dtm_m` by applying `as.matrix()` to `ag_dtm`.
* Using brackets, `[` and `]`, index `ag_dtm_m` to row `2206`.
* Apply `tidy()` to `ag_dtm`. Call the new object `ag_tidy`.
* Examine `ag_tidy` at rows `[831:835, ]` to compare the tidy format. You will see a common word from the examined part of `ag_dtm_m` in step 2.

```{r}
# data prep
agamemnon <- readLines("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/Agamemnon.txt")
aga_short <- agamemnon[(str_which(agamemnon, "CHARACTERS IN THE PLAY") + 1):(str_which(agamemnon, "NOTES TO THE AGAMEMNON") - 1)]
aga_short <- aga_short[aga_short != ""]
aga_vec <- VectorSource(aga_short)
aga_corp <- VCorpus(aga_vec)
aga_clean <- clean_corpus(aga_corp)
content(aga_clean[[1]])
ag_dtm <- DocumentTermMatrix(aga_clean)
ag_txt <- aga_short 

# one question would be the difference between all this process and tidytext::unnest_tokens()

ag_dtm_m <- as.matrix(ag_dtm)
ag_dtm_m[2106, 245:250]

library(broom)
ag_tidy <- tidy(ag_dtm)
ag_tidy[831:835,]
```

## Examine the sentiments data frame

So far you have used a single lexicon. Now we will transition to using three, each measuring sentiment in different ways.

The `tidytext` package contains a data frame called `sentiments`. The data frame contains over 23000 terms from three different subjectivity lexicons with corresponding information. Here are some example rows from the `sentiments` data frame.

**Word** |	**Sentiment** |	**Lexicon**	 | **Score**
---|---|---|---
abhorrent |	NA |	AFINN	 |-3
cool |	NA |	AFINN |	1
congenial |	positive |	Bing |	NA
enemy	| negative |	Bing	| NA
ungrateful | anger	| NRC	| NA
sectarian|	anger	| NRC	| NA

Notice the tidy format. Each word is a row and NAs fill in columns that are not applicable. The "AFINN" lexicon scores words from 5 to -5. The "Bing" lexicon is the same lexicon used in `qdap`'s `polarity()` function. "Bing" words are only labeled as positive or negative. The "NRC" lexicon has distinct emotional classes covering Plutchik's Wheel and positive and negative.

Let's explore the sentiments data frame in more detail!

#### Instructions
* Use `get_sentiments()` to obtain the `"afinn"` lexicon, assigning to `afinn_lex`.
* Review the overall `count()` of score in `afinn_lex`.
* Do the same again, this time with the `"nrc"` lexicon. That is,
  + get the sentiments, assigning to `nrc_lex`, then
  + count the `sentiment` column, assigning to `nrc_counts`.
* Create a ggplot of `n` vs. `sentiment`.
* Add a `col` layer using `geom_col()`. (This is like `geom_bar()`, but used when you've already summarized with `count()`.)


```{r}
afinn_lex <- get_sentiments("afinn")

(affin_counts <- afinn_lex %>% 
  count(score))

nrc_lex <- get_sentiments("nrc")

(nrc_counts <- nrc_lex %>% 
    count(sentiment))

nrc_counts %>% ggplot(aes(x = sentiment, y = n)) +
  geom_col() +
  theme_gdocs()
```

## Bing tidy polarity: Simple example

The Bing lexicon labels words as positive or negative. The next three exercises let you interact with this specific lexicon. Instead of using `filter()` to extract a lexicon this exercise uses `get_sentiments()` which accepts a string such as "afinn", "bing", "nrc", or "loughran".

Now that you understand the basics of an inner join, let's apply this to the "Bing" lexicon. Keep in mind the `inner_join()` function comes from `dplyr` and the `sentiments` object is from `tidytext`.

The inner join workflow:

* Obtain the correct lexicon using either `filter()` or `get_sentiments()`.
* Pass the lexicon and the tidy text data to `inner_join()`.
* In order for `inner_join()` to work there must be a shared column name. If there are no shared column names, declare them with an additional parameter, `by` equal to `c` with column names like below.

> object <- x %>%  
>    inner_join(y, by = c("column_from_x" = "column_from_y")

* Perform some aggregation and analysis on the table intersection.

#### Instructions
We've loaded `ag_txt` containing the first 100 lines from Agamemnon and `ag_tidy` which is the tidy version.

* For comparison, use `polarity()` on `ag_txt`.
* Get the `"bing"` lexicon by passing that string to `get_sentiments()`.
* Perform an `inner_join()` with `ag_tidy` and `bing`.
  + The word columns are called `"term"` in `ag_tidy` and `"word"` in the lexicon, so you need to specify the by argument.
  + Call the new object `ag_bing_words`.
* Print `ag_bing_words`, and look at some of the words that are in the result.
* Pass `ag_bing_words` to `count()` of sentiment using the pipe operator, `%>%`. Compare the `polarity()` score to sentiment count ratio.

```{r}
(ag_pol <- polarity(ag_txt))

bing <- get_sentiments("bing")
ag_bing_words <- ag_tidy %>% 
  inner_join(bing, by = c("term" = "word"))

ag_bing_words

ag_bing_words %>% 
  count(sentiment)
```

## Bing tidy polarity: Count & spread the white whale

In this exercise you will apply another `inner_join()` using the `"bing"` lexicon.

Then you will manipulate the results with both `count()` from `dplyr` and `spread()` from `tidyr` to learn about the text.

The `spread()` function spreads a key-value pair across multiple columns. In this case key is the sentiment and the values are the frequency of positive or negative terms for each line. Using `spread()` changes the data so that each row now has positive and negative values, even if it is 0.

#### Instructions
In this exercise, your R session has `m_dick_tidy` which contains the book Moby Dick and `bing`, containing the lexicon similar to the previous exercise.

* Perform an `inner_join()` on `m_dick_tidy` and bing.
  + As before, join the `"term"` column in `m_dick_tidy` to the `"word"` column in the lexicon.
  + Call the new object `moby_lex_words`.
* Create a column `index`, equal to `as.numeric()` applied to `document`. This occurs within `mutate()` in the tidyverse.
* Create `moby_count` by forwarding `moby_lex_words` to `count()`, passing in `sentiment`, `index`.
* Generate `moby_spread` by piping `moby_count` to `spread()` which contains sentiment, `n`, and `fill = 0`.

```{r}
# data prep
all_books <- readRDS("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/all_books.rds")
mb_book <- all_books %>% filter(book == "moby_dick")
m_dick_tidy <- mb_book %>% select(document, term, count)

moby_lex_words <- m_dick_tidy %>% 
  inner_join(get_sentiments("bing"), by = c("term" = "word"))


moby_lex_words <- moby_lex_words %>% mutate(index = as.numeric(document))

(moby_count <- moby_lex_words %>% 
    count(sentiment, index))

# Spreading the dataframe 
library(tidyr)
(moby_spread <- moby_count %>% 
    spread(sentiment, n, fill = 0))
```

## Bing tidy polarity: Call me Ishmael (with ggplot2)!

The last Bing lexicon exercise! In this exercise you will use the pipe operator (`%>%`) to create a timeline of the sentiment in Moby Dick. In the end you will also create a simple visual following the code structure below. The next chapter goes into more depth for visuals.

> ggplot(spread_data, aes(index_column, polarity_column)) +  
>  geom_smooth()

#### Instructions
* Inner join moby to the `bing` lexicon.
  + Call `inner_join()` to join the tibbles.
  + Join by the `term` column in the text and the `word` column in the lexicon.
* Count by `sentiment` and `index`.
* Reshape so that each `sentiment` has its own column.
  + Call `spread()`.
  + The key column (to split into multiple columns) is `sentiment`.
  + The value column (containing the counts) is `n`.
  + Also specify `fill = 0` to fill out missing values with a zero.
* Use `mutate()` to add the `polarity` column. Define it as the difference between the `positive` and `negative` columns.
* Using `moby_polarity`, plot `polarity` vs. `index`.
* Add a smooth trend layer by calling `geom_smooth()` with no arguments.

```{r}
# data prep 
moby <- mb_book %>% 
  mutate(index = as.numeric(document)) %>% 
  select(term, count, index)

(moby_polarity <- moby %>% 
    inner_join(get_sentiments("bing"), by = c("term" = "word")) %>% 
    count(sentiment, index) %>% 
    spread(sentiment, n, fill = 0) %>% 
    mutate(polarity = positive - negative))

library(ggplot2)

moby_polarity %>% ggplot(aes(x = index, y = polarity)) +
  geom_smooth()
```

## AFINN: I'm your Huckleberry

Now we transition to the AFINN lexicon. The AFINN lexicon has numeric values from 5 to -5, not just positive or negative. Unlike the Bing lexicon's `sentiment`, the AFINN lexicon's sentiment score column is called `score`.

As before, you apply `inner_join()` then `count()`. Next, to sum the scores of each line, we use `dplyr`'s `group_by()` and `summarize()` functions. The `group_by()` function takes an existing data frame and converts it into a grouped data frame where operations are performed "by group". Then, the `summarize()` function lets you calculate a value for each group in your data frame using a function that aggregates data, like `sum()` or `mean()`. So, in our case we can do something like

> data_frame %>%   
>    group_by(book_line) %>%   
>    summarize(total_score = sum(book_line))

In the tidy version of *Huckleberry Finn*, line 9703 contains words "best", "ever", "fun", "life" and "spirit". "best" and "fun" have AFINN scores of 3 and 4 respectively. After aggregating, line 9703 will have a total score of 7.

In the tidyverse, `filter()` is preferred to `subset()` because it combines the functionality of `subset()` with simpler syntax. Here is an example that `filter()`s `data_frame` where some value in `column1` is equal to `24`. Notice the column name is not in quotes.

> filter(data_frame, column1 == 24)

The `afinn` object contains the AFINN lexicon. The `huck` object is a tidy version of Mark Twain's Adventures of Huckleberry Finn for analysis.

Line 5400 is *All the loafers looked glad; I reckoned they was used to having fun out of Boggs. Stopwords and punctuation have already been removed in the dataset*.

#### Instructions
* Run the code to look at line 5400, and see the sentiment scores of some words.
* `inner_join()` huck to the afinn lexicon.
  + Remember `huck` is already piped into the function so just add the lexicon.
  + Join by the `term` column in the text and the `word` column in the lexicon.
* Use `count()` with score and line to tally/count observations by group.
* Assign the result to `huck_afinn`.
* Get the total sentiment score by line forwarding huck_afinn to `group_by()` and passing line without quotes.
  + Create `huck_afinn_agg` using `summarize()`, setting `total_score` equal to the `sum()` of score * `n`.
* Use `filter()` on `huck_afinn_agg` and `line == 5400` to review a single line.
* Create a sentiment timeline. Pass `huck_afinn_agg` to the data argument of `ggplot()`.
* Then specify the `x` and `y` within `aes()` as line and `total_score` without quotes.
* Add a layer with `geom_smooth()`.

```{r}
# data prep
huck <- all_books %>% 
  filter(book == "huck_finn") %>% 
  mutate(line = as.numeric(document)) %>% 
  select(term, count, line)

huck %>% filter(line == 5400)
afinn <- get_sentiments("afinn")

# What are the scores of the sentiment words?
afinn %>% filter(word %in% c("fun", "glad"))

huck_afinn <- huck %>%
  inner_join(afinn, by = c("term" = "word")) %>% 
  count(score, line)

huck_afinn_agg <- huck_afinn %>% 
  group_by(line) %>% 
  summarise(total_score = sum(score * n))

huck_afinn_agg %>% filter(line == 5400)

huck_afinn_agg %>%
  ggplot(aes(x = line, y = total_score)) +
  geom_smooth()
```

## The wonderful wizard of NRC

Last but not least, you get to work with the NRC lexicon which labels words across multiple emotional states. Remember Plutchik's wheel of emotion? The NRC lexicon tags words according to Plutchik's 8 emotions plus positive/negative.

In this exercise there is a new operator, `%in%`, which matches a vector to another. In the code below `%in%` will return `FALSE`, `FALSE`, `TRUE`. This is because within some_vec, 1 and 2 are not found within some_other_vector but 3 is found and returns `TRUE`. The %in% is useful to find matches.

> some_vec <- c(1, 2, 3)  
> some_other_vector <- c(3, "a", "b")  
> some_vec %in% some_other_vector

Another new operator is `!`. For logical conditions, adding `!` will inverse the result. In the above example, the `FALSE`, `FALSE`, `TRUE` will become `TRUE`, `TRUE`, `FALSE`. Using it in concert with `%in%` will inverse the response and is good for removing items that are matched.

> !some_vec %in% some_other_vector

We've created `oz` which is the tidy version of `The Wizard of Oz` along with `nrc` containing the "NRC" lexicon with renamed columns.

#### Instructions
* Inner join `oz` to the `nrc` lexicon.
  + Call `inner_join()` to join the tibbles.
  + Join `by` the `term` column in the text and the `word` column in the lexicon.
* Filter to only Pluchik's emotions and drop the positive or negative words in the lexicon.
  + Use `filter()` to keep rows where the sentiment is not `"positive"` or `"negative"`.
* Group by sentiment.
  + Call `group_by()`, passing sentiment without quotes.
* Get the total count of each sentiment.
  + Call `summarize()`, setting `total_count` equal to the `sum()` of count.
  + Assign the result to `oz_plutchik`
* Create a bar plot with `ggplot()`.
  + Pass in `oz_plutchik` to the `data` argument.
  + Then specify the `x` and `y` aesthetics, calling `aes()` and passing `sentiment` and `total_count` without quotes.
  + Add a column geom with `geom_col()`. (This is the same as `geom_bar()`, but doesn't summarize the data, since you've done that already.)

```{r}
# data prep
plaintxt <- readLines("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/oz.txt")
startstr <- "START OF THIS PROJECT GUTENBERG EBOOK THE WONDERFUL WIZARD OF OZ"
endstr <- "End of Project Gutenberg's The Wonderful Wizard of Oz, by L. Frank Baum"
plaintxt_short <- plaintxt[(stringr::str_which(plaintxt, startstr) + 1):(stringr::str_which(plaintxt, endstr) - 1)]
plaintxt_short <- plaintxt_short[plaintxt_short != ""]
txt_vec <- tm::VectorSource(plaintxt_short)
txt_corp <- tm::VCorpus(txt_vec)
txt_clean <- clean_corpus(txt_corp)
txt_dtm <- tm::DocumentTermMatrix(txt_clean)
oz_tidy <- tidytext::tidy(txt_dtm)
oz <- oz_tidy

##################################################################################
# Tidytext way to do something similar [probably with less text cleaning]
plaintxt_short_df <- as.data.frame(plaintxt_short)
plaintxt_short_df$plaintxt_short <- as.character(plaintxt_short_df$plaintxt_short)
z2 <- plaintxt_short_df %>%  
  mutate(linenumber = row_number()) %>% 
  unnest_tokens(output = "word", input = plaintxt_short) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = T)
##################################################################################

oz_plutchik <- oz %>% 
  inner_join(get_sentiments("nrc"), by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>% 
  group_by(sentiment) %>% 
  summarize(total_count = sum(count))

oz_plutchik %>% ggplot(aes(x = sentiment, y = total_count)) +
  geom_col()
```

# 3rd Segment - Visualizing sentiment

## Unhappy ending? Chronological polarity

Sometimes you want to track sentiment over time. For example, during an ad campaign you could track brand sentiment to see the campaign's effect. You saw a few examples of this at the end of the last chapter.

In this exercise you'll recap the workflow for exploring sentiment over time using the novel Moby Dick. One should expect that happy moments in the book would have more positive words than negative. Conversely dark moments and sad endings should use more negative language. You'll also see some tricks to make your sentiment time series more visually appealling.

Recall that the workflow is:

1. Inner join the text to the lexicon by word.
2. Count the sentiments by line.
3. Reshape the data so each sentiment has its own column.
4. (Depending upon the lexicon) Calculate the polarity as positive score minus negative score.
5. Draw the polarity time series.

This exercise should look familiar: it extends Bing tidy polarity: Call me Ishmael (with ggplot2)!.

#### Instructions
* `inner_join()` the pre-loaded tidy version of Moby Dick, `moby`, to the `bing` lexicon.
  + Join by the `"term"` column in the text and the `"word"` column in the lexicon.
* Count by sentiment and index.
* Reshape so that each sentiment has its own column using spread() with the column sentiment and the counts column called n.
  + Also specify `fill = 0` to fill out missing values with a zero.
* Using mutate() add two columns: polarity and line_number.
Set polarity equal to the positive score minus the negative score.
Set line_number equal to the row number using the row_number() function.
* Create a sentiment time series with `ggplot()`.
  + Pass in `moby_polarity` to the `data` argument.
  + Call `aes()` and pass in `line_number` and `polarity` without quotes.
  + Add a smoothed curve with `geom_smooth()`.
  + Add a red horizontal line at zero by calling `geom_hline()`, with parameters `0` and `"red"`.
  + Add a title with `ggtitle()` set to `"Moby Dick Chronological Polarity"`

```{r}
moby_polarity <- moby %>% 
  inner_join(get_sentiments("bing"), by = c("term" = "word")) %>% 
  count(sentiment, index) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(polarity = positive - negative, line_number = row_number())

moby_polarity %>% ggplot(aes(x = line_number, y = polarity)) +
  geom_smooth() +
  geom_hline(yintercept = 0, col = "red") +
  ggtitle("Moby Dick Chronological Polarity") + 
  theme_gdocs()
```

## Word impact, frequency analysis

One of the easiest ways to explore data is with a frequency analysis. Although not difficult, in sentiment analysis this simple method can be surprisingly illuminating. Specifically, you will build a barplot. In this exercise you are once again working with `moby` and `bing` to construct your visual.

**To get the bars ordered from lowest to highest, you will use a trick with factors**. `reorder()` lets you change the order of factor levels based upon another scoring variable. In this case, you will reorder the factor variable term by the scoring variable polarity.

#### Instructions
* Create `moby_tidy_sentiment`.
  - inner join with `bing`
  - Use `count()` with `term`, `sentiment`, and `wt = count`.
  - Pipe to `spread()` with `sentiment`, `n`, and `fill = 0`.
  - Pipe to `mutate()`. Call the new variable `polarity`; calculated as positive minus negative.
* Call `moby_tidy_sentiment` to review and compare it to the previous exercise.
* Use `filter()` on `moby_tidy_sentiment` to keep rows where the absolute polarity is greater than or equal to 50. `abs()` gives you absolute values.
* `mutate()` a new vector `pos_or_neg` with an `ifelse()` function checking if `polarity > 0` then declare the document `"positive"` else declare it `"negative"`.
* Using `moby_tidy_pol`, plot `polarity` vs. `term`, reordered by `polarity` `(reorder(term, polarity)`), `fill`ed by `pos_or_neg`. ass `geom_col()` and title.
* Inside `element_text()`, rotate the x-axis text `90` degrees by setting `angle = 90` and shifting the vertical justification with `vjust = -0.1`.

```{r}
moby_tidy_sentiment <- moby %>% 
  inner_join(get_sentiments("bing"), by = c("term" = "word")) %>% 
  count(term, sentiment, wt = count) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(polarity = positive - negative)

moby_tidy_pol <- moby_tidy_sentiment %>%
  filter(abs(polarity) >= 50) %>% 
  mutate(pos_or_neg = ifelse(polarity > 0, "positive", "negative"))

moby_tidy_pol %>% ggplot(aes(x = reorder(term, polarity), y = polarity, fill = pos_or_neg)) +
  geom_col() +
  ggtitle("Moby Dick: Sentiment Word Frequency") + 
  theme_gdocs() +
  theme(axis.text.x = element_text(angle = 90, vjust = -0.1))
```

## Divide & conquer: Using polarity for a comparison cloud

Now that you have seen how polarity can be used to divide a corpus, let's do it! This code will walk you through dividing a corpus based on sentiment so you can peer into the informaton in subsets instead of holistically.

Your R session has `oz_pol` which was created by applying `polarity()` to "The Wonderful Wizard of Oz."

For simplicity's sake, we created a simple custom function called `pol_subsections()` which will divide the corpus by polarity score. First, the function accepts a data frame with each row being a sentence or document of the corpus. The data frame is subset anywhere the polarity values are greater than or less than 0. Finally, the positive and negative sentences, non-zero polarities, are pasted with parameter collapse so that the terms are grouped into a single corpus. Lastly, the two documents are concatenated into a single vector of two distinct documents.

> pol_subsections <- function(df) {  
>   x.pos <- subset(df$text, df$polarity > 0)  
>   x.neg <- subset(df$text, df$polarity < 0)  
>   x.pos <- paste(x.pos, collapse = " ")  
>   x.neg <- paste(x.neg, collapse = " ")  
>   all.terms <- c(x.pos, x.neg)  
>  return(all.terms)    
> }

At this point you have omitted the neutral sentences and want to focus on organizing the remaining text. In this exercise we use the `%>%` operator again to forward objects to functions. After some simple cleaning use `comparison.cloud()` to make the visual.

#### Instructions
* Extract the bits you need from `oz_pol`.
* Call `select()`, declaring the first column `text` as `text.var` which is the raw text. The second column `polarity` should refer to the polarity scores `polarity`.
* Now apply `pol_subsections()` to `oz_df`. Call the new object `all_terms`.
* To create `all_corpus` apply `VectorSource()` to `all_terms` and then `%>%` to `VCorpus()`.
* Create a term-document matrix, `all_tdm`, using `TermDocumentMatrix()` on `all_corpus`.
  + Add in the parameters 
   `control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en")))`.
  + Then `%>%` to `as.matrix()` and `%>%` again to `set_colnames(c("positive", "negative"))`.

```{r}
# data prep
plaintxt <- readLines("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/oz.txt")
startstr <- "START OF THIS PROJECT GUTENBERG EBOOK THE WONDERFUL WIZARD OF OZ"
endstr <- "End of Project Gutenberg's The Wonderful Wizard of Oz, by L. Frank Baum"
plaintxt_short <- plaintxt[(stringr::str_which(plaintxt, startstr) + 1):(stringr::str_which(plaintxt, endstr) - 1)]
oz <- plaintxt_short[plaintxt_short != ""]
oz_pol <- polarity(oz)

# this functions splits a dataframe of text, polarity in 2 subgroups of positive and negative polarities
pol_subsections <- function(df) {  
 x.pos <- subset(df$text, df$polarity > 0)  
 x.neg <- subset(df$text, df$polarity < 0)  
 x.pos <- paste(x.pos, collapse = " ")  
 x.neg <- paste(x.neg, collapse = " ")  
 all.terms <- c(x.pos, x.neg)  
return(all.terms)    
}

library(magrittr)
library(wordcloud)

#oz_pol is an object of polarity class. oz_pol$all$text.var contains the text by line. 
oz_df <- oz_pol$all %>% 
  select(text = text.var, polarity = polarity)

#all_terms contain the two collapsed character strings based on positive or negative polarity
all_terms <- pol_subsections(oz_df)
  
# on the fly VCorpus creation with pipes
all_corpus <- all_terms %>% 
  VectorSource() %>% 
  VCorpus()

# terms are split according to frequency in positive lines and negative lines
all_tdm <- all_corpus %>% 
  TermDocumentMatrix(control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en"))) %>% 
  as.matrix() %>% 
  set_colnames(c("positive", "negative"))

# this could be all piped from the beginning!
all_tdm %>% comparison.cloud(max.words = 50, colors = c("darkgreen", "darkred"))
```

## Emotional introspection

In this exercise you go beyond subsetting on positive and negative language. Instead you will subset text by each of the 8 emotions in Plutchik's emotional wheel to construct a visual. With this approach you will get more clarity in word usage by mapping to a specific emotion instead of just positive or negative.

Using the `tidytext` subjectivity lexicon, `"nrc"`, you perform an `inner_join()` with your text. The `"nrc"` lexicon has the 8 emotions plus positive and negative term classes. So you will have to drop positive and negative words after performing your `inner_join()`. One way to do so is with the negation, `!`, and `grepl()`.

The "Global Regular Expression Print Logical" function, `grepl()`, will return a `True` or `False` if a string pattern is identified in each row. In this exercise you will search for positive OR negative using the `|` operator, representing "or" as shown below. Often this straight line is above the enter key on a keyboard. Since the `!` negation precedes `grepl()`, the `T` or `F` is switched so the "positive|negative" is dropped instead of kept.

> Object <- tibble %>%
>   filter(!grepl("positive|negative", column_name))

Next you apply `count()` on the identified words along with `spread()` to get the data frame organized.

`comparison.cloud()` requires its input to have row names, so you'll have to convert it to a base-R `data.frame`, calling `data.frame()` with the `row.names` argument.

#### Instructions
* `inner_join()` `moby` to `nrc`.
* Using `filter()` with a negation (`!`) and `grepl()` search for "positive|negative". The column to search is called `sentiment`.
* Use `count()` to count by `sentiment` and `term`.
* Reshape the data frame with `spread()`, passing in `sentiment`, `n`, and `fill = 0`.
* Convert to plain data frame with `data.frame()`, making the `term` column into `rownames`.
* Examine `moby_tidy` using `head()`.
* Using `moby_tidy`, draw a `comparison.cloud()`.
  + Limit to `50` words.
  + Increase the title size to `1.5`.

```{r}
# data_prep
moby <- moby %>% select(document = index, term, count)

moby_tidy <- moby %>% 
  inner_join(get_sentiments("nrc"), by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>%  # I don't see why we should use grepl for this???
  count(sentiment, term) %>% 
  spread(sentiment, n, fill = 0) %>% #here it is a tibble
  data.frame(row.names = "term")

head(moby_tidy)

moby_tidy %>% comparison.cloud(max.words = 50, title.size = 1.5)
```

## Compare & contrast stacked bar chart

Another way to slice your text is to understand how much of the document(s) are made of positive or negative words. For example a restaurant review may have some positive aspects such as "the food was good" but then continue to add "the restaurant was dirty, the staff was rude and parking was awful." As a result, you may want to understand how much of a document is dedicated to positive vs negative language. In this example it would have a higher negative percentage compared to positive.

One method for doing so is to `count()` the positive and negative words then divide by the number of subjectivity words identified. In the restaurant review example, "good" would count as 1 positive and "dirty," "rude," and "awful" count as 3 negative terms. A simple calculation would lead you to believe the restaurant review is 25% positive and 75% negative since there were 4 subjectivity terms.

Start by performing the `inner_join()` on a unified tidy data frame containing 4 books, Agamemnon, Oz, Huck Finn, and Moby Dick. Just like the previous exercise you will use `filter()` and `grepl()`.

To perform the `count()` you have to group the data by book and then sentiment. For example all the positive words for Agamemnon have to be grouped then tallied so that positive words from all books are not mixed. Luckily, you can pass multiple variables into `count()` directly.

#### Instructions
* Inner join `all_books` to the lexicon, `nrc`.
* Filter to keep rows where sentiment contains "positive" or "negative". That is, use `grepl()` on the sentiment column, checking without the negation so that `"positive|negative"` are kept.
* Count by `book` and `sentiment`.
* Group `books_sent_count` by `book`.
* Mutate to add a column named `percent`. This should be calculated as 100 times `n` divided by the sum of `n`.
* Using `book_pos`, plot `percent` vs. `book`, using `sentiment` as the fill color.
* Add a column layer with `geom_col()`.

```{r}
# data prep 
all_books <-  all_books %>% 
  select(document, term, count, book)

tail(all_books)

books_sent_count <- all_books %>% 
  inner_join(get_sentiments("nrc"), by = c("term" = "word")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(book, sentiment)

book_pos <- books_sent_count %>% 
  group_by(book) %>% 
  mutate(percent = 100 * n / sum(n))

book_pos %>% ggplot(aes(x = book, y = percent, fill = sentiment)) +
  geom_col()
```

## Kernel density plot

Now that you learned about a kernel density plot you can create one! Remember it's like a smoothed histogram but isn't affected by binwidth. This exercise will help you construct a kernel density plot from sentiment values.

In this exercise you will plot 2 kernel densities. One for Agamemnon and another for The Wizard of Oz. For both you will perform an `inner_join()` with the `"afinn"` lexicon. Recall the `"afinn"` lexicon has terms scored from -5 to 5. Once in a tidy format, both books will retain words and corresponding scores for the lexicon.

After that, you need to row bind the results into a larger data frame using `bind_rows()` and create a plot with `ggplot2`.

From the visual you will be able to understand which book uses more positive versus negative language. There is clearly overlap as negative things happen to Dorothy but you could infer the kernel density is demonstrating a greater probability of positive language in the Wizard of Oz compared to Agamemnon.

We've loaded `ag` and `oz` as tidy versions of Agamemnon and The Wizard of Oz respectively, and created `afinn` as a subset of the tidytext `"afinn"` lexicon.

#### Instructions
* Inner join `ag` to the lexicon, `afinn`, assigning to `ag_afinn`.
* Do the same for The Wizard of Oz. This is the same code, but starting with the `oz` dataset and assigning to `oz_afinn`.
* Use `bind_rows()` to combine `ag_afinn` to `oz_afinn`. Set the `.id` argument to `"book"` to create a new column with the name of each book.
* Using `all_df`, plot score, using `book` as the `fill` color.
* Set the `alpha` transparency to `0.3`.

```{r}
# data prep
ag <- ag_tidy

ag_afinn <- ag %>% 
  inner_join(get_sentiments("afinn"), by = c("term" = "word"))

oz_afinn <- oz %>% 
  inner_join(get_sentiments("afinn"), by = c("term" = "word"))

all_df <- bind_rows(agamemnon = ag_afinn, oz = oz_afinn, .id = "book") # this ways it correctly labels the new "book" column

all_df %>% ggplot(aes(x = score, fill = book)) + 
  geom_density(alpha = 0.3) + 
  theme_gdocs() +
  ggtitle("AFINN Score Densities")
```

## Box plot

An easy way to compare multiple distributions is with a box plot. This code will help you construct multiple box plots to make a compact visual.

In this exercise the `all_book_polarity` object is already loaded. The data frame contains two columns, `book` and `polarity`. It comprises all books with `qdap`'s `polarity()` function applied. Here are the first 3 rows of the large object.

 ""  | book |	polarity 
--|--|--
14 |	huck	| 0.2773501
22 |	huck	| 0.2581989
26 |	huck  |	-0.5773503

This exercise introduces `tapply()` which allows you to apply functions over a ragged array. You input a vector of values and then a vector of factors. For each factor, value combination the third parameter, a function like min(), is applied. For example here's some code with tapply() used on two vectors.

> f1 <- as.factor(c("Group1", "Group2", "Group1", "Group2"))  
> stat1 <- c(1, 2, 1, 2)  
> tapply(stat1, f1, sum)  

The result is an array where Group1 has a value of 2 (1+1) and Group2 has a value of 4 (2+2).

#### Instructions
* Since it's already loaded, examine the `all_book_polarity` with `str()`.
* Using `tapply()`, pass in `all_book_polarity$polarity`, `all_book_polarity$book` and the `summary()` function. This will print the summary statistics for the 4 books in terms of their `polarity()` scores. You would expect to see Oz and Huck Finn to have higher averages than Agamemnon or Moby Dick. Pay close attention to the median.
* Create a box plot with `ggplot()` by passing in `all_book_polarity`.
  + Aesthetics should be `aes(x = book, y = polarity)`.
  + Using a + add the `geom_boxplot()` with `col = "darkred"`. Pay close attention to the dark line in each box representing median.
  + Next add another layer called `geom_jitter()` to add points for each of the words.
  
```{r}
all_book_polarity <- readRDS("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/all_book_polarity.rds")

str(all_book_polarity)

tapply(all_book_polarity$polarity, all_book_polarity$book, summary) #summary on polarity grouped by book.

all_book_polarity %>% ggplot(aes(x = book, y = polarity)) +
  geom_boxplot(fill = c("#bada55", "#F00B42", "#F001ED", "#BA6E15"), col = "darkred") +
  geom_jitter(position = position_jitter(width = 0.1, height = 0), alpha = 0.02) +
  theme_gdocs() +
  ggtitle("Book Polarity")
```
  
## Radar chart

Remember Plutchik's wheel of emotion? The NRC lexicon has the 8 emotions corresponding to the first ring of the wheel. Previously you created a `comparison.cloud()` according to the 8 primary emotions. Now you will create a radar chart similar to the wheel in this exercise.

A `radarchart` is a two-dimensional representation of multidimensional data (at least 3). In this case the tally of the different emotions for a book are represented in the chart. Using a radar chart, you can review all 8 emotions simultaneously.

As before we've loaded the `"nrc"` lexicon as `nrc` and `moby_huck` which is a combined tidy version of both Moby Dick and Huck Finn.

In this exercise you once again use a negated `grepl()` to remove "positive|negative" emotional classes from the chart. As a refresher here is an example:

> object <- tibble %>%  
>   filter(!grepl("positive|negative", column_name))

This exercise reintroduces `spread()` which rearranges the tallied emotional words. As a refresher consider this raw data called `datacamp`.

people	| food	| like
--|--|--
Nicole	| bread	| 78
Nicole	| salad	| 66
Ted | bread	| 99
Ted |	salad	| 21

If you applied `spread()` as in `spread(datacamp, people, like)` the data looks like this.

food |	Nicole |	Ted
--|--|--
bread |	78 |	99
salad |	66 |	21

#### Instructions
* Review `moby_huck` with `tail()`.
* `inner_join()` `moby_huck` and `nrc`.
* Next, `filter()` negating "positive|negative" in the `sentiment` column. Assign the result to `books_pos_neg`.
* After `books_pos_neg` is forwarded to `group_by()` with `book` and `sentiment`. Then `tally()` the object with an empty `*` function.
* Then `spread()` the `books_tally` by the `book` and `n` column.
* Review the `scores` data.
* Call `chartJSRadar()` on scores which is an `htmlwidget` from the `radarchart` package.

```{r}
# data prep
moby_huck <- all_books %>%  
  filter(book %in% c("moby_dick", "huck_finn")) %>% 
  select(book, document, term, count)

tail(moby_huck)

books_pos_neg <- moby_huck %>%
  inner_join(get_sentiments("nrc"), by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative"))

books_pos_neg %>% 
  group_by(book, sentiment) %>% 
  tally() %>% 
  spread(book, n, fill = 0)

## similar to
scores <- books_pos_neg %>% 
  count(book, sentiment) %>% 
  spread(book, n, fill = 0)

scores

# in one shot
scores <- moby_huck %>%
  inner_join(get_sentiments("nrc"), by = c("term" = "word")) %>% 
  filter(!sentiment %in% c("positive", "negative")) %>% 
  count(book, sentiment) %>% 
  spread(book, n, fill = 0)

scores

library(radarchart)

chartJSRadar(scores)
```

## Treemaps for groups of documents

Often you will find yourself working with documents in groups, such as author, product or by company. This exercise lets you learn about the text while retaining the groups in a compact visual. For example, with customer reviews grouped by product you may want to explore multiple dimensions of the customer reviews at the same time. First you could calculate the `polarity()` of the reviews. Another dimension may be length. Document length can demonstrate the emotional intensity. If a customer leaves a short "great shoes!" one could infer they are actually less enthusiastic compared to a lengthier positive review. You may also want to group reviews by product type such as women's, men's and children's shoes. A **treemap** lets you examine all of these dimensions.

For text analysis, within a treemap each individual box represents a document such as a tweet. Documents are grouped in some manner such as author. The size of each box is determined by a numeric value such as number of words or letters. The individual colors are determined by a sentiment score.

After you organize the tibble, you use the [`treemap`](https://www.rdocumentation.org/packages/treemap/versions/2.4-2) library containing the function `treemap()` to make the visual. The code example below declares the data, grouping variables, size, color and other aesthetics.

> treemap(  
>  data_frame,  
>  index = c("group", "individual_document"),  
>  vSize = "doc_length",  
>  vColor = "avg_score",  
>  type = "value",  
>  title = "Sentiment Scores by Doc",  
>  palette = c("red", "white", "green")  
> )

The pre-loaded `all_books` object contains a combined tidy format corpus with 4 Shakespeare, 3 Melville and 4 Twain books. Based on the treemap you should be able to tell who writes longer books, and the polarity of the author as a whole and for individual books.  

#### Instructions
* Calculate each book's length in a new object called `book_length` using `count()` with the book column.
* Inner join `all_books` to the lexicon, `afinn`.
* Group by `author` and `book`.
* Use `summarize()` to calculate the `mean_score` as the `mean()` of `score`.
* Inner join again, this time to `book_length`. Join by the book column.
* Draw a treemap, setting the following arguments.
  + Use the `book_tree` from the previous step.
  + Specify the aggregation `index` columns as `"author"` and `"book"`.
  + Specify the vertex size column, `vSize`, as `"n"`.
  + Specify the vertex color column, `vColor`, as `"mean_score"`.
  + Specify a direct mapping from `vColor` to the palette by setting `type = "value"`.

```{r}
# data prep
all_books <- readRDS("/Users/lbarcelo/R_Repo/DataCamp/Sentiment Analysis in R/Data/all_books.rds")

book_length <- all_books %>% 
  count(book)

book_tree <- all_books %>% 
  inner_join(get_sentiments("afinn"), by = c("term" = "word")) %>% 
  group_by(author, book) %>% 
  summarise(mean_score = mean(score)) %>% 
  inner_join(book_length)

book_tree

library(treemap)

treemap(dtf = book_tree,
        index = c("author", "book"),
        vSize = "n",
        vColor = "mean_score",
        type = "value",
        title = "Book Sentiment Scores",
        palette = c("red", "white", "green")
        )
```

# 4th Segment - Case study: Airbnb reviews
