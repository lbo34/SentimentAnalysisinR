---
title: "Sentiment Analysis in R"
subtitle: "DataCamp Course by Ted Kwartler"
author: "Laurent Barcelo"
date: "September 25, 2017"
output: 
  html_notebook:
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
```

# 1st Segment - Fast & Dirty: Polarity scoring

## Jump right in! Visualize polarity

Sentiment analysis helps you extract an author's feelings towards a subject. This exercise will give you a taste of what's to come!

We created `text_df` representing a conversation with `person` and `text` columns.

Use `qdap`'s `polarity()` function to score `text_df`. `polarity()` will accept a single character object or data frame with a grouping variable to calculate a positive or negative score.

In this example you will use the `magrittr` package's dollar pipe operator `%$%`. The dollar sign forwards the data frame into `polarity()` and you declare a text column name or the text column and a grouping variable without quotes.

> text_data_frame %$% polarity(text_column_name)

To create an object with the dollar sign operator:

> polarity_object <- text_data_frame %$%   
>  polarity(text_column_name, grouping_column_name)

More specifically, to make a quantitative judgement about the sentiment of some text, you need to give it a score. A simple method is a positive or negative value related to a sentence, passage or a collection of documents called a corpus. Scoring with positive or negative values only is called "polarity." A useful function for extracting polarity scores is `counts()` applied to the polarity object. For a quick visual call `plot()` on the `polarity()` outcome.

#### Instructions
* Examine the `text_df` conversation data frame.
* Using `%$%` pass `text_df` to `polarity()` along with the column name text without quotes. This will print the polarity for all text.
* Create a new object `datacamp_conversation` by forwarding `text_df` with `%$%` to `polarity()`. Pass in text followed by the grouping person column. This will calculate polarity according to each individual person. Since it is all within parentheses the result will be printed too.
* Apply `counts()` to datacamp_conversation to print the specific emotional words that were found.
* `plot()` the datacamp_conversation.

```{r echo = F}
text_df <- data.frame(matrix(NA, nrow = 8, ncol = 2))
text_df[,1] <- as.factor(c("Nick ", "Jonathan", "Martijn",  "Nicole", "Nick", "Jonathan", "Martijn", "Nicole"))
text_df[,2] <- as.factor(c("DataCamp courses are the best", "I like talking to students", "Other online data science curricula are boring.", "What is for lunch?", "DataCamp has lots of great content!", "Students are passionate and are excited to learn", "Other data science curriculum is hard to learn and difficult to understand", "I think the food here is good."))
colnames(text_df) <- c("person", "text")
```

```{r}
library(magrittr)
library(qdap)
library(tidyverse)
text_df

text_df %$% polarity(text)

# Calc polarity score by person
(datacamp_conversation <- text_df %$% polarity(text, person))

counts(datacamp_conversation)

plot(datacamp_conversation)
```

## TM refresher (I)

In the Text Mining: Bag of Words course you learned that a corpus is a set of texts, and you studied some functions for preprocessing the text. To recap, one way to create a corpus is with the functions below. Even though this is a different course, sentiment analysis is part of text mining so a refresher can be helpful.

* Turn a character vector into a text source using `VectorSource()`.
* Turn a text source into a corpus using `VCorpus()`.
* Remove unwanted characters from the corpus using cleaning functions like `removePunctuation()` and `stripWhitespace()` from `tm`, and `replace_abbreviation()` from `qdap`.

In this exercise a custom `clean_corpus()` function has been created using standard preprocessing functions for easier application.

`clean_corpus()` accepts the output of `VCorpus()` and applies cleaning functions. For example:

> processed_corpus <- clean_corpus(my_corpus)

#### Instructions
Your R session has a text vector, `tm_define`, containing two small documents and the function `clean_corpus()`.

* Create an object called `tm_vector` by applying `VectorSource()` to `tm_define`.
* Make `tm_corpus` using `VCorpus()` on `tm_vector`.
* Use `content()` to examine the contents of the first document in `tm_corpus`.
  + Documents in the corpus are accessed using list syntax, so use double square brackets, e.g. `[[1]]`.
* Clean the corpus text using the custom function `clean_corpus()` on `tm_corpus`. Call this new object `tm_clean`.
* Examine the first document of the new `tm_clean` object again to see how the text changed after `clean_corpus()` was applied.

```{r}
# data prep
library(tm)
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee"))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

tm_define <- c("Text mining is the process of distilling actionable insights from text.", "Sentiment analysis represents the set of tools to extract an author's feelings towards a subject.")

tm_vector <- VectorSource(tm_define)
tm_corpus <- VCorpus(tm_vector)
content(tm_corpus[[1]])
tm_clean <- clean_corpus(tm_corpus)
content(tm_clean[[1]])
```

## TM refresher (II)

Now let's create a Document Term Matrix (DTM). In a DTM:

* Each row of the matrix represents a document.
* Each column is a unique word token.
* Values of the matrix correspond to an individual document's word usage.

The DTM is the basis for many bag of words analyses. Later in the course, you will also use the related Term Document Matrix (TDM). This is the transpose; that is, columns represent documents and rows represent unique word tokens.

You should construct a DTM after cleaning the corpus (using `clean_corpus()`). To do so, call `DocumentTermMatrix()` on the corpus object.

> tm_dtm <- DocumentTermMatrix(tm_clean

If you need a more in-depth refresher check out the Text Mining: Bag of Words course. Hopefully these two exercises have prepared you well enough to embark on your sentiment analysis journey!

#### Instructions
We've created a `VCorpus()` object called `clean_text` containing 1000 tweets mentioning coffee. The tweets have been cleaned with the previously mentioned preprocessing steps and your goal is to create a DTM from it.

* Apply `DocumentTermMatrix()` to the `clean_text` corpus to create a term frequency weighted DTM called `tf_dtm`.
* Change the `DocumentTermMatrix()` object into a simple matrix with `as.matrix()`. Call the new object `tf_dtm_m`.
* Check the dimensions of the matrix using `dim()`.
* Use square bracket indexing to see a subset of the matrix.
  + Select rows 16 to 20, and columns 2975 to 2985
* Note the frequency value of the word "working."

```{r}
# data prep
coffee <- read_csv("/Users/lbarcelo/R_Repo/Rdatasets/DataCamp/Text Minings - Bags of Words/coffee.csv")
coffee_tweets <- coffee$text
coffee_source <- VectorSource(coffee_tweets)
coffee_corpus <- VCorpus(coffee_source)
clean_text <- clean_corpus(coffee_corpus)

clean_text
tf_dtm <- DocumentTermMatrix(clean_text)
tf_dtm_m <- as.matrix(tf_dtm)
dim(tf_dtm_m)
tf_dtm_m[16:20, 2975:2985]
tf_dtm_m[1:200, "working"]
```

## Where can you observe Zipf's law?

Although Zipf observed a steep and predictable decline in word usage you may not buy into Zipf's law. You may be thinking "I know plenty of words, and have a distinctive vocabulary". That may be the case, but the same can't be said for most people! To prove it, let's construct a visual from 3 million tweets mentioning "#sb". Keep in mind that the visual doesn't follow Zipf's law perfectly, the tweets all mentioned the same hashtag so it is a bit skewed. That said, the visual you will make follows a steep decline showing a small lexical diversity among the millions of tweets. So there is some science behind using lexicons for natural language analysis!

In this exercise, you will use the package `metricsgraphics`. Although the author suggests using the pipe `%>%` operator, you will construct the graphic step-by-step to learn about the various aspects of the plot. The main function of the package `metricsgraphics` is the `mjs_plot()` function which is the first step in creating a JavaScript plot. Once you have that, you can add other layers on top of the plot.

An example `metricsgraphics` workflow without using the `%>%` operator is below:

> metro_plot <- mjs_plot(data, x = x_axis_name, y = y_axis_name, show_rollover_text = FALSE)  
> metro_plot <- mjs_line(metro_plot)  
> metro_plot <- mjs_add_line(metro_plot, line_one_values)  
> metro_plot <- mjs_add_legend(metro_plot, legend = c('names', 'more_names'))  
> metro_plot

#### Instructions
* Use `head()` on sb_words to review top words.
* Create a new column `expectations` by dividing the largetst word frequency, `freq[1]`, by the `rank` column.
* Start `sb_plot` using `mjs_plot()`.
  + Pass in `sb_words` with `x = rank` and `y = freq`.
  + Within `mjs_plot()` set `show_rollover_text` to `FALSE`.
* Overwrite `sb_plot` using `mjs_line()` and pass in `sb_plot`.
* Add to `sb_plot` with `mjs_add_line()`.
  + Pass in the previous `sb_plot` object and the vector, `expectations`.
* Place a legend on a new `sb_plot` object using `mjs_add_legend()`.
  + Pass in the previous sb_plot object
  + The legend labels should consist of `"Frequency"` and `"Expectation"`.
* Call `sb_plot` to display the plot. Mouseover a point to simultaneously highlight a `freq` and `Expectation` point. The magic of JavaScript!

```{r}
# Data (sb_words not available)
# Examine sb_words
head(sb_words)

# Create expectations
sb_words$expectations <- sb_words %$% {freq[1] / rank}

# Create metrics plot
sb_plot <- mjs_plot(sb_words, x = rank, y = freq, show_rollover_text = F)

# Add 1st line
sb_plot <- mjs_line(sb_plot)
####
# Add 2nd line
sb_plot <- mjs_add_line(sb_plot, expectations)

# Add legend
sb_plot <- mjs_add_legend(sb_plot, legend = c("Frequency", "Expectation"))

# Display plot
sb_plot
```

## Polarity on actual text

So far you have learned the basic components needed for assessing positive or negative intent in text. Remember the following points so you can feel confident in your results.

The subjectivity lexicon is a predefined list of words associated with emotions or positive/negative feelings.
You don't have to list every word in a subjectivity lexicon because Zipf's law describes human expression.
A quick way to get started is to use the `polarity()` function which has a built-in subjectivity lexicon.

The function scans the text to identify words in the lexicon. It then creates a word group around the identified positive or negative subjectivity word. Within the group **valence shifters** adjust the score. Valence shifters are words that amplify or negate the emotional intent of the subjectivity word. For example, "well known" is positive while "not well known" is negative. Here "not" is a negating term and reverses the emotional intent of "well known." In contrast, "very well known" employs an amplifier increasing the positive intent.

The `polarity()` function then calculates a score using subjectivity terms, valence shifters and the total number of words in the passage. This exercise demonstrates a simple polarity calculation. In the next video we look under the hood of `polarity()` for more detail.

#### Instructions
* Calculate the `polarity()` of positive in a new object called pos_score. Encase the entire call in parentheses so the output is also printed.

Manually perfrom the same polarity calculation.

* Get a word count object by calling `counts()` on the polarity object.
* All the identified subjectivity words are part of count object's list. Specifically, positive words are in `$pos.words` element vector. Find the number of positive words in `n_good` by calling `length()` on the first part of the `$pos.words` element.
* Capture the total number of words and assign it to n_words. This value is stored in pos_count as the wc element.
* Deconstruct the polarity() calculation by dividing n_good by sqrt() of n_words. Compare the result to pos_pol to the equation's result.

```{r}
# Example statement
positive <- "DataCamp courses are good for learning"

# Calculate polarity of statement
(pos_score <- polarity(positive))

(pos_counts <- counts(pos_score))

# Number of positive words
n_good <- length(pos_counts$pos.words[[1]])

# Total number of words
n_words <- pos_counts$wc
  
# Verify polarity score
n_good / sqrt(n_words)
```

## Happy songs!

Of course just positive and negative words aren't enough. In this exercise you will learn about valence shifters which tell you about the author's emotional intent. Previously you applied `polarity()` to text without valence shifters. In this example you will see amplifers and negating words in action.

Recall that an amplifying word adds 0.8 to a positive word in `polarity()` so the positive score becomes 1.8. For negative words 0.8 is subtracted so the total becomes -1.8. Then the score is divided by the square root of the total number of words.

Consider the following example from Frank Sinatra:

* **"It was a very good year"**

"Good" equals 1 and "very" adds another 0.8. So, 1.8/sqrt(6) results in 0.73 polarity.

A negating word such as "not" will inverse the subjectivity score. Consider the following example from Bobby McFerrin:

* **"Don't worry Be Happy"**

"worry is now 1 due to the negation "don't." Adding the "happy", +1, equals 2. With 4 total words, 2 / sqrt(4) equals a polarity score of 1.

#### Instructions
* Examine the conversation data frame, `conversation`. Note the valence shifters like `"never"` in the text column.
* Apply `polarity()` to the text column of conversation to calculate polarity for the entire conversation.
* Calculate the polarity scores by student, assigning the result to `student_pol`.
  + Call `polarity()` again, this time passing two columns of conversation.
  + The text variable is text and the grouping variable is student.
* To see the student level results, use `scores()` on `student_pol`.
* The `counts()` function applied to `student_pol` will print the sentence level polarity for the entire data frame along with lexicon words identified.
* The polarity object, `student_pol`, can be plotted with `plot()`.

```{r}
# data prep
conversation <- data.frame(matrix(NA, nrow = 3, ncol = 2))
conversation[,1] <- as.factor(c("Martijn", "Nick",  "Nicole"))
conversation[,2] <- as.factor(c("This restaurant is never bad", "The lunch was very good", "It was awful I got food poisoning and was extremely ill"))
colnames(conversation) <- c("student", "text")

conversation

conversation %$% polarity(text)
(student_pol <- conversation %$% polarity(text, student))

scores(student_pol) # same as printing it?

counts(student_pol)

plot(student_pol)
```

## LOL, this song is wicked good

Even with Zipf's law in action, you will still need to adjust lexicons to fit the text source (for example twitter versus legal documents) or the author's demographics (teenage girl versus middle aged man). This exercise demonstrates the explicit components of `polarity()` so you can change it if needed.

In Trey Songz "Lol :)" song there is a lyric "LOL smiley face, LOL smiley face." In the basic `polarity()` function, "LOL" is not defined as positive. However, "LOL" stands for "Laugh Out Loud" and should be positive. As a result, you should adjust the lexicon to fit the text's context which includes pop-culture slang. If your analysis contains text from a specific channel (Twitter's "LOL"), location (Boston's "Wicked Good"), or age group (teenagers "sick") you will likely have to adjust the lexicon.

In this exercise you are not adjusting the subjectivity lexicon or qdap dictionaries containing valence shifters. Instead you are examining the existing word data frame objects so you can change them in the following exercise.

We've created `text` containing two excerpts from BeyoncÃ©'s "Crazy in Love" lyrics for the exercise.

#### Instructions
* Print `key.pol` to see a portion of the subjectivity words and values.
* Examine the predefined `negation.words` to print all the negating terms.
* Now print the `amplification.words` to see the words that add values to the lexicon.
* Check the `deamplification.words` to print the words that reduce the lexicon values.
* Call `text` to see conversation.
* Calculate polarity() as follows.
  + Set `text.var` to `text$words`.
  + Set `grouping.var` to `text$speaker`.
  + Set `polarity.frame` to `key.pol`.
  + Set `negators` to `negation.words`.
  + Set `amplifiers` to `amplification.words`.
  + Set `deamplifiers` to `deamplification.words`.

```{r}
# Examine the key.pol
key.pol

# Negators
negation.words

# Amplifiers
amplification.words

# De-amplifiers
deamplification.words

# data prep
text <- data.frame(matrix(NA, nrow = 2, ncol = 2))
text[,1] <- as.factor(c("beyonce", "jay_z"))
text[,2] <- as.factor(c("I know I dont understand Just how your love can do what no one else can", "They cant figure him out they like hey, is he insane"))
colnames(text) <- c("speaker", "words")

text

polarity(text.var = text$words,
         grouping.var = text$speaker,
         polarity.frame = key.pol,
         negators = negation.words,
         amplifiers = amplification.words,
         deamplifiers = deamplification.words)

```

## Stressed Out!

Here you will adjust the negative words to account for the specific text. You will then compare the basic and custom `polarity()` scores.

A popular song from Twenty One Pilots is called "Stressed Out". If you scan the lyrics of this song, you will observe the song is about youthful nostalgia. Overall, most people would say the polarity is negative. Repeatedly the lyrics mention stress, fears and pretending.

Let's compare the song lyrics using the default subjectivity lexicon and also a custom one.

To start, you need to verify the `key.pol` subjectivity lexicon does not already have the term you want to add. One way to check is with `grep()`. The `grep()` function returns the row containing characters that match a search pattern. Here is an example used while indexing.

> data_frame[grep("search_pattern", data_frame$column), ]

After verifying the slang or new word is not already in the `key.pol` lexicon you need to add it. The code below uses `sentiment_frame()` to construct the new lexicon. Within the code `sentiment_frame()` accepts the original positive word vector, `positive.words`. Next, the original `negative.words` are concatenated to "smh" and "kappa", both considered negative slang. Although you can declare the positive and negative weights, the default is 1 and -1 so they are not included below.

> custom_pol <- sentiment_frame(positive.words, c(negative.words, "hate", "pain"))

Now you are ready to apply polarity and it will reference the *custom* subjectivity lexicon!

#### Instructions
We've created `stressed_out` which contains the lyrics to the song "Stressed Out", by Twenty One Pilots.

* Use `polarity()` on stressed_out to see the default score.
* Check `key.pol` for any words containing "stress". Use `grep()` to index the data frame by searching in the `x` column.
* Create `custom_pol` as a new sentiment data frame.
  + Call `sentiment_frame()` and pass `positive.words` as the first argument without concatenating any new terms.
  + Next, use `c()` to combine `negative.words` with new terms `"stressed"` and `"turn back"`.
* Reapply `polarity()` to `stressed_out` with the additional parameter `polarity.frame = custom_pol` to compare how the new words change the score to a more accurate representation of the song.

```{r}
# data prep
stressed_out <- "I wish I found some better sounds no ones ever heard\nI wish I had a better voice that sang some better words\nI wish I found some chords in an order that is new\nI wish I didnt have to rhyme every time I sang\nI was told when I get older all my fears would shrink\nBut now Im insecure and I care what people think\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWere stressed out\nSometimes a certain smell will take me back to when I was young\nHow come Im never able to identify where its coming from\nId make a candle out of it if I ever found it\nTry to sell it never sell out of it Id probably only sell one\nItd be to my brother, cause we have the same nose\nSame clothes homegrown a stones throw from a creek we used to roam\nBut it would remind us of when nothing really mattered\nOut of student loans and tree-house homes we all would take the latter\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWe used to play pretend, give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face #\nSaying, Wake up you need to make money\nYeah\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nUsed to play pretend, used to play pretend bunny\nWe used to play pretend wake up, you need the money\nUsed to play pretend used to play pretend bunny\nWe used to play pretend, wake up, you need the money\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah"

head(stressed_out)

polarity(stressed_out)

str_view_all(key.pol, pattern = "stress", match = T)
```